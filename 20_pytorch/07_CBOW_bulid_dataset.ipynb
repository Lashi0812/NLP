{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import Iterator\n",
    "\n",
    "# torch\n",
    "from torchdata import datapipes as dp\n",
    "from torchdata.datapipes import functional_datapipe\n",
    "from torchdata.datapipes.iter import IterDataPipe\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # data\n",
    "    ## Flag\n",
    "    path_check = True,\n",
    "    is_content = False,\n",
    "    url = \"https://www.gutenberg.org/files/84/84-0.txt\",\n",
    "    ## path and file name\n",
    "    data_base_path = \"../data/cbow/\",\n",
    "    filename = \"frankenstein.txt\",\n",
    "    start_tkn = \"*** START OF THE PROJECT GUTENBERG EBOOK\",\n",
    "    end_tkn = \"*** END OF THE PROJECT GUTENBERG EBOOK \",\n",
    ")\n",
    "\n",
    "if args.path_check:\n",
    "    for k,v in args._get_kwargs():\n",
    "        if k.endswith(\"path\"):\n",
    "            Path(v).mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def para_joiner_fn(line):\n",
    "    return \" \".join(line)\n",
    "\n",
    "def drop_filename_fn(tuples):\n",
    "    return tuples[1]\n",
    "\n",
    "def filter_content(args,para):\n",
    "    if args.start_tkn in para:\n",
    "        args.is_content = True\n",
    "    elif args.is_content and args.end_tkn in para:\n",
    "        args.is_content = False\n",
    "    return args.is_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "http_reader_dp  = dp.iter.HttpReader([args.url])\n",
    "line_reader_dp  = http_reader_dp.readlines(encoding=\"utf-8-sig\",decode=True)\n",
    "para_dp = line_reader_dp.lines_to_paragraphs(para_joiner_fn)\n",
    "drop_dp = para_dp.map(drop_filename_fn)\n",
    "filter_dp = drop_dp.filter(partial(filter_content,args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    with open(args.data_base_path+args.filename , \"w\") as f:\n",
    "        for data in filter_dp:\n",
    "            f.write(data+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@Language.component(\"preprocess\")\n",
    "def preprocess(doc):\n",
    "    words = []\n",
    "    space = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha or token.text in [\".\",\"?\",\"!\",\",\"]:\n",
    "            words.append(token.lower_)\n",
    "            space.append(True)\n",
    "    return Doc(vocab=nlp.vocab,words=words,spaces=space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x19972b42380>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"preprocess\")\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functional_datapipe(\"get_sentence\")\n",
    "class SentencePipe(IterDataPipe):\n",
    "    def __init__(self,datapipe,nlp) -> None:\n",
    "        super().__init__()\n",
    "        self.datapipe = datapipe\n",
    "        self.nlp = nlp \n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for para in self.datapipe:\n",
    "            doc = nlp(para)\n",
    "            for sent in doc.sents:\n",
    "                if len(sent)>2:\n",
    "                    yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functional_datapipe(\"get_context_target\")\n",
    "class ContextTargetPipe(IterDataPipe):\n",
    "    def __init__(self,datapipe,window_size=2) -> None:\n",
    "        super().__init__()\n",
    "        self.datapipe = datapipe\n",
    "        self.window_size = window_size\n",
    "\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for sent in self.datapipe:\n",
    "            for i,target in enumerate(sent):\n",
    "                yield (sent[max(0,i-self.window_size):i].text_with_ws + sent[i+1:i+self.window_size+1].text_with_ws,\n",
    "                       target.text)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opener = dp.iter.FileOpener([\"../data/cbow/frankenstein.txt\"])\n",
    "reader =  opener.readlines(return_path= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dp = reader.get_sentence(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start of the project gutenberg ebook frankenstein"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(sent_dp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_dp = sent_dp.get_context_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83244"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(con_dp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = con_dp.random_split(weights={\"train\":0.7,\"test\":0.15,\"val\":0.15},\n",
    "                            seed=0,total_length=83244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname,pipe in zip([\"train\",\"test\",\"val\"],split):\n",
    "    with open(args.data_base_path+f\"{fname}.csv\",\"w\") as f:\n",
    "        for data in pipe:\n",
    "            f.write(f'{data[0].strip()}#{data[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('start of project gutenberg ', 'the')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(split[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
