{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import (\n",
    "    OrderedDict,\n",
    "    Counter,\n",
    "    defaultdict\n",
    ")\n",
    "\n",
    "# torch \n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence\n",
    "from torch.utils.data import default_collate\n",
    "# other \n",
    "from torchtext import vocab\n",
    "try:\n",
    "    from torchdata import datapipes as dp\n",
    "    from torchdata.dataloader2 import DataLoader2\n",
    "except:\n",
    "    !pip install torchdata\n",
    "    from torchdata import datapipes as dp\n",
    "    from torchdata.dataloader2 import DataLoader2\n",
    "\n",
    "# manipulation \n",
    "import numpy as np\n",
    "\n",
    "#other\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # data\n",
    "    data_base_path = \"../../data/mt/\",\n",
    "    dataset = [\"train_1\"],\n",
    "    \n",
    "    # vocab\n",
    "    mask_tkn = \"<MASK>\",\n",
    "    ukn_tkn = \"<UKN>\",\n",
    "    beg_tkn = \"<BEG>\",\n",
    "    end_tkn = \"<END>\",\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 2,\n",
    "    embedding_size = 5,\n",
    "    rnn_hidden_size = 4,\n",
    "    \n",
    "    \n",
    "    # runtime\n",
    "    cuda = torch.cuda.is_available(),\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \n",
    ")\n",
    "for k,v in args._get_kwargs():\n",
    "    if \"base\" in k:\n",
    "        Path(v).mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipe_dict(args=args):\n",
    "    pipe_dict = {}\n",
    "    for fname in args.dataset:\n",
    "        pipe = dp.iter.FileOpener([args.data_base_path+f\"{fname}.csv\"])\n",
    "        pipe_dict[fname] = pipe.parse_csv(skip_lines=1)\n",
    "    return pipe_dict          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_1': CSVParserIterDataPipe}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_dict = build_pipe_dict(args)\n",
    "pipe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(counter,max_seq_length,args):\n",
    "    sort_tuples = sorted(counter.items(),key=lambda kf :(-kf[1],kf[0]))\n",
    "    vocabulary = vocab.vocab(ordered_dict=OrderedDict(sort_tuples),\n",
    "                        specials=[args.mask_tkn,\n",
    "                                  args.ukn_tkn,\n",
    "                                  args.beg_tkn,\n",
    "                                  args.end_tkn])\n",
    "    vocabulary.max_seq_length = max_seq_length\n",
    "    vocabulary.set_default_index(vocabulary[args.ukn_tkn])\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_dict(train_pipe,args=args):\n",
    "    counter_dict = defaultdict(Counter)\n",
    "    max_seq_len_dict = defaultdict(int)\n",
    "    for row in train_pipe:\n",
    "        for key,sent in zip([\"source\",\"target\"],row):\n",
    "            max_seq_len_dict[key] = max(max_seq_len_dict[key],len((sent_list:=sent.split(\" \"))))\n",
    "            counter_dict[key].update(sent_list)\n",
    "    \n",
    "    return {k:create_vocab(counter_dict[k],max_seq_len_dict[k],args)\n",
    "            for k in counter_dict.keys()}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': Vocab(), 'target': Vocab()}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = build_vocab_dict(pipe_dict[\"train_1\"],args)\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(indices,seq_length,mask_idx):\n",
    "    vector = np.full(shape=seq_length,\n",
    "                     fill_value=mask_idx,\n",
    "                     dtype=np.int64)\n",
    "    vector[:len(indices)] = indices\n",
    "    return vector\n",
    "\n",
    "def get_source_indices(source_text,source_vocab,args=args):\n",
    "    indices = [source_vocab[args.beg_tkn]]\n",
    "    indices.extend(source_vocab.lookup_indices(source_text.split(\" \")))\n",
    "    indices.append(source_vocab[args.end_tkn])\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def get_target_indices(target_text,target_vocab,args=args):\n",
    "    indices = target_vocab.lookup_indices(target_text.split(\" \"))\n",
    "    target_x = [target_vocab[args.beg_tkn]] + indices\n",
    "    target_y = indices + [target_vocab[args.end_tkn]]\n",
    "    return target_x ,target_y\n",
    "\n",
    "def create_dataset(vocab_dict,args,row):\n",
    "    source_indices = get_source_indices(source_text=row[0],\n",
    "                                        source_vocab=vocab_dict[\"source\"],\n",
    "                                        args=args)\n",
    "    source_vector = vectorize(source_indices,\n",
    "                              seq_length = vocab_dict[\"source\"].max_seq_length + 2,\n",
    "                              mask_idx= vocab_dict[\"source\"][args.mask_tkn])\n",
    "    \n",
    "    target_x_indices ,target_y_indices = get_target_indices(target_text=row[1],\n",
    "                                                            target_vocab=vocab_dict[\"target\"],\n",
    "                                                            args=args)\n",
    "    \n",
    "    target_x_vector = vectorize(target_x_indices,\n",
    "                              seq_length = vocab_dict[\"target\"].max_seq_length + 1,\n",
    "                              mask_idx= vocab_dict[\"target\"][args.mask_tkn])\n",
    "    \n",
    "    target_y_vector = vectorize(target_y_indices,\n",
    "                              seq_length = vocab_dict[\"target\"].max_seq_length + 1,\n",
    "                              mask_idx= vocab_dict[\"target\"][args.mask_tkn])\n",
    "    \n",
    "    return {\"source_vector\":source_vector,\n",
    "            \"target_x_vector\":target_x_vector,\n",
    "            \"target_y_vector\":target_y_vector,\n",
    "            \"source_length\":len(source_indices)}\n",
    "    \n",
    "    \n",
    "def collate_fn(args,batch):\n",
    "    data_dict = default_collate(batch)\n",
    "    lengths = data_dict[\"source_length\"].numpy()\n",
    "    sorted_lengths = lengths.argsort()[::-1].tolist()\n",
    "    \n",
    "    return {k:v[sorted_lengths].to(args.device)\n",
    "            for k,v in data_dict.items()}\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_pipe(pipe_dict,vocab_dict,args):\n",
    "    dataset_pipe = {}\n",
    "    fn = partial(create_dataset,vocab_dict,args)\n",
    "    for dataset,pipe in pipe_dict.items():\n",
    "        if dataset == \"train\":\n",
    "            pipe = pipe.shuffle()\n",
    "        \n",
    "        pipe = pipe.map(fn)\n",
    "        pipe = pipe.batch(args.batch_size,drop_last=True)\n",
    "        pipe = pipe.collate(partial(collate_fn,args))\n",
    "        \n",
    "        dataset_pipe[dataset] = pipe\n",
    "        \n",
    "    return dataset_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_1': CollatorIterDataPipe}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = build_dataset_pipe(pipe_dict,vocab_dict,args)\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.source_vocab_size = len(vocab_dict[\"source\"])\n",
    "args.target_vocab_size = len(vocab_dict[\"target\"])\n",
    "args.padding_idx = vocab_dict[\"source\"][args.mask_tkn]\n",
    "args.target_embedding_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<MASK>',\n",
       " '<UKN>',\n",
       " '<BEG>',\n",
       " '<END>',\n",
       " '.',\n",
       " 'always',\n",
       " 'am',\n",
       " 'complaining',\n",
       " 'exhausted',\n",
       " 'her',\n",
       " 'i',\n",
       " 'is',\n",
       " 'job',\n",
       " 'of',\n",
       " 'she']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"source\"].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<MASK>',\n",
       " '<UKN>',\n",
       " '<BEG>',\n",
       " '<END>',\n",
       " '.',\n",
       " 'de',\n",
       " 'elle',\n",
       " 'je',\n",
       " 'plaint',\n",
       " 'se',\n",
       " 'son',\n",
       " 'suis',\n",
       " 'toujours',\n",
       " 'travail',\n",
       " 'vannÃ©']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"target\"].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader2(dataset_dict[\"train_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_vector': tensor([[ 2, 14, 11,  5,  7, 13,  9, 12,  4,  3],\n",
       "         [ 2, 10,  6,  8,  4,  3,  0,  0,  0,  0]]),\n",
       " 'target_x_vector': tensor([[ 2,  6,  9,  8, 12,  5, 10, 13,  4],\n",
       "         [ 2,  7, 11, 14,  4,  0,  0,  0,  0]]),\n",
       " 'target_y_vector': tensor([[ 6,  9,  8, 12,  5, 10, 13,  4,  3],\n",
       "         [ 7, 11, 14,  4,  3,  0,  0,  0,  0]]),\n",
       " 'source_length': tensor([10,  6])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(dataset_dict[\"train_1\"]))\n",
    "sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1. take in input [ B , Seq ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_input = sample[\"source_vector\"]\n",
    "source_input.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we have two input with each input have fixed seq length of 10.\n",
    "2. Each element hold the word index in the vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Convert input vector into its embedding [ B , Seq , Emb]\n",
    "\n",
    "we creating the embedding nothing but the lookup where index are word ,then each each word are represent as the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_embedding = nn.Embedding(num_embeddings=args.source_vocab_size,\n",
    "                                embedding_dim=args.embedding_size,padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(source_embedding(source_input) == source_embedding.weight[source_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_emb_out = source_embedding(source_input)\n",
    "source_emb_out.shape\n",
    "\n",
    "#? each word is represent as the 5 dim vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Encode the emb using bi_gru\n",
    "\n",
    "1. output shape ==> [B,Seq,D*hid]\n",
    "2. Hidden shape ==> [D*layers,B,hid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(5, 4, bias=False, batch_first=True, bidirectional=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bi_gru = nn.GRU(input_size=args.embedding_size,\n",
    "                        hidden_size = args.rnn_hidden_size,\n",
    "                        bias=False,\n",
    "                        batch_first=True,\n",
    "                        num_layers=1,\n",
    "                        bidirectional=True)\n",
    "encoder_bi_gru"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bi-GRU will have the four weights.\n",
    "2. Two for the forward and Two for the backward.\n",
    "3. Each GRU Cell has three gate reset,update,new gate\n",
    "4. Each Gate have two weighs one for input and another for hidden.\n",
    "5. Shape of each gate weights for input [hidden size ,input size] . Three Weight matrix concat to form Weight matrix for input_hidden ,Shape [3*hidden,input_size]\n",
    "6. shape of each gate weights for hidden [hidden_size,hidden_size].Same will happen to these matrix will form the hidden_hidden weight matrix of shape [3*hidden,hidden] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 4]), torch.Size([12, 4]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_hh_f = encoder_bi_gru.weight_hh_l0\n",
    "W_hh_b = encoder_bi_gru.weight_hh_l0_reverse\n",
    "\n",
    "W_hh_f.shape,W_hh_b.shape\n",
    "\n",
    "#? [3(#gate)*4(hidden_size),4(hidden_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 5]), torch.Size([12, 5]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_ih_f = encoder_bi_gru.weight_ih_l0\n",
    "W_ih_b = encoder_bi_gru.weight_ih_l0_reverse\n",
    "\n",
    "W_ih_f.shape,W_ih_b.shape\n",
    "\n",
    "#? [3(#gate)*4(hidden_size),5(input_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_ir_f,W_iz_f,W_in_f = W_ih_f.split(args.rnn_hidden_size)\n",
    "W_hr_f,W_hz_f,W_hn_f = W_hh_f.split(args.rnn_hidden_size)\n",
    "W_ir_b,W_iz_b,W_in_b = W_ih_b.split(args.rnn_hidden_size)\n",
    "W_hr_b,W_hz_b,W_hn_b = W_hh_b.split(args.rnn_hidden_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have split the each weight into its gate weights matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hidden(xt,h,W_ir,W_iz,W_in,W_hr,W_hz,W_hn):\n",
    "    rt = torch.sigmoid(xt@W_ir.T + h@W_hr.T)\n",
    "    zt = torch.sigmoid(xt@W_iz.T + h@W_hz.T)\n",
    "    nt = torch.tanh(xt@W_in.T + rt*(h@W_hn.T))\n",
    "    h = (1-zt)*nt + zt*h\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros(size=(2,args.batch_size,args.rnn_hidden_size))\n",
    "\n",
    "forward = []\n",
    "backward = []\n",
    "hf = h[0].unsqueeze(0)\n",
    "hb = h[1].unsqueeze(0)\n",
    "#? we have to iterate over sequence so permute to [seq,batch,emb]\n",
    "source_seq_input = source_emb_out.permute(1,0,2)\n",
    "for t in range(source_seq_input.size(0)):\n",
    "    xt_f = source_seq_input[t]\n",
    "    xt_b = source_seq_input[-1-t]\n",
    "    hf = compute_hidden(xt_f,hf,W_ir_f,W_iz_f,W_in_f,W_hr_f,W_hz_f,W_hn_f)\n",
    "    hb = compute_hidden(xt_b,hb,W_ir_b,W_iz_b,W_in_b,W_hr_b,W_hz_b,W_hn_b)\n",
    "    forward.append(hf)\n",
    "    backward.append(hb)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_h = torch.cat((hf,hb),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_out = torch.cat((torch.stack(forward).squeeze(),\n",
    "                        torch.flip(torch.stack(backward).squeeze(),dims=[0])), # flip across the seq dim so that we can concat the forward and backward\n",
    "                       dim=2).permute(1,0,2)\n",
    "\n",
    "#? concat across the embedding \n",
    "#? then we permute to batch,seq,emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9074, -2.1085, -0.3591,  1.1981, -0.3069],\n",
       "         [-0.8585, -0.1450,  1.7102,  1.5498,  0.8394],\n",
       "         [ 0.6814,  1.2009,  2.8156, -0.8746,  0.4696],\n",
       "         [-0.8103,  0.8221, -0.8722, -1.5284, -0.3509],\n",
       "         [-0.6266, -0.0082, -1.1014,  0.9016,  0.5329],\n",
       "         [-1.1077, -0.7007,  2.5961,  0.6664,  0.8542],\n",
       "         [ 0.5068, -1.8874, -0.2909, -0.1333,  1.1500],\n",
       "         [ 0.1781,  0.2571,  1.3376,  0.9025,  0.5470],\n",
       "         [-0.6205, -1.5771,  0.1747, -0.3322, -0.2763],\n",
       "         [ 0.2494, -0.2989, -0.3181,  1.2363,  0.7480]],\n",
       "\n",
       "        [[ 0.9074, -2.1085, -0.3591,  1.1981, -0.3069],\n",
       "         [ 0.7964, -0.1401,  0.6662, -0.4248,  0.1476],\n",
       "         [-0.3629, -0.9660,  0.8563,  0.7797, -1.0582],\n",
       "         [-1.1474,  1.1108,  0.7414, -0.3431, -0.3324],\n",
       "         [-0.6205, -1.5771,  0.1747, -0.3322, -0.2763],\n",
       "         [ 0.2494, -0.2989, -0.3181,  1.2363,  0.7480],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_emb_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_out , encoder_h = encoder_bi_gru(source_emb_out,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.isclose(encoder_h ,manual_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.isclose(encoder_out ,manual_out,atol=1e-7))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3A Using packed sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.9074, -2.1085, -0.3591,  1.1981, -0.3069],\n",
       "        [ 0.9074, -2.1085, -0.3591,  1.1981, -0.3069],\n",
       "        [-0.8585, -0.1450,  1.7102,  1.5498,  0.8394],\n",
       "        [ 0.7964, -0.1401,  0.6662, -0.4248,  0.1476],\n",
       "        [ 0.6814,  1.2009,  2.8156, -0.8746,  0.4696],\n",
       "        [-0.3629, -0.9660,  0.8563,  0.7797, -1.0582],\n",
       "        [-0.8103,  0.8221, -0.8722, -1.5284, -0.3509],\n",
       "        [-1.1474,  1.1108,  0.7414, -0.3431, -0.3324],\n",
       "        [-0.6266, -0.0082, -1.1014,  0.9016,  0.5329],\n",
       "        [-0.6205, -1.5771,  0.1747, -0.3322, -0.2763],\n",
       "        [-1.1077, -0.7007,  2.5961,  0.6664,  0.8542],\n",
       "        [ 0.2494, -0.2989, -0.3181,  1.2363,  0.7480],\n",
       "        [ 0.5068, -1.8874, -0.2909, -0.1333,  1.1500],\n",
       "        [ 0.1781,  0.2571,  1.3376,  0.9025,  0.5470],\n",
       "        [-0.6205, -1.5771,  0.1747, -0.3322, -0.2763],\n",
       "        [ 0.2494, -0.2989, -0.3181,  1.2363,  0.7480]],\n",
       "       grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([2, 2, 2, 2, 2, 2, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_emb_packed = pack_padded_sequence(source_emb_out,\n",
    "                                         sample[\"source_length\"].numpy().tolist(),batch_first=True)\n",
    "source_emb_packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9074, -2.1085, -0.3591,  1.1981, -0.3069],\n",
       "         [-0.8585, -0.1450,  1.7102,  1.5498,  0.8394],\n",
       "         [ 0.6814,  1.2009,  2.8156, -0.8746,  0.4696],\n",
       "         [-0.8103,  0.8221, -0.8722, -1.5284, -0.3509],\n",
       "         [-0.6266, -0.0082, -1.1014,  0.9016,  0.5329],\n",
       "         [-1.1077, -0.7007,  2.5961,  0.6664,  0.8542],\n",
       "         [ 0.5068, -1.8874, -0.2909, -0.1333,  1.1500],\n",
       "         [ 0.1781,  0.2571,  1.3376,  0.9025,  0.5470],\n",
       "         [-0.6205, -1.5771,  0.1747, -0.3322, -0.2763],\n",
       "         [ 0.2494, -0.2989, -0.3181,  1.2363,  0.7480]],\n",
       "\n",
       "        [[ 0.9074, -2.1085, -0.3591,  1.1981, -0.3069],\n",
       "         [ 0.7964, -0.1401,  0.6662, -0.4248,  0.1476],\n",
       "         [-0.3629, -0.9660,  0.8563,  0.7797, -1.0582],\n",
       "         [-1.1474,  1.1108,  0.7414, -0.3431, -0.3324],\n",
       "         [-0.6205, -1.5771,  0.1747, -0.3322, -0.2763],\n",
       "         [ 0.2494, -0.2989, -0.3181,  1.2363,  0.7480],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_emb_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_emb_packed.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pack_encoder_out,pack_encoder_h = encoder_bi_gru(source_emb_packed,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_encoder_out,_ = pad_packed_sequence(pack_encoder_out,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(padded_encoder_out ,encoder_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. False in above output is happen only in forward sequence only not on the backward sequence.\n",
    "2. All false in the padded seq position.\n",
    "\n",
    "> Why this is happened only the padded position only?\n",
    ">>  When we pass the shorter seq to gru , it learn something in the non-padded position ,this learned info is passed in padded position also.but in the packed sequence there is no padded sequence , there it wont pass the info, it remain zero . this is what we needed.**So alway use the packed sequence when training rnn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  True],\n",
       "         [False, False, False, False]],\n",
       "\n",
       "        [[ True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(pack_encoder_h ,encoder_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1182,  0.5731,  0.2595, -0.5787, -0.4998, -0.0218,  0.4067, -0.2413],\n",
       "        [-0.0100,  0.2469, -0.0475, -0.3790, -0.3101,  0.1612,  0.1219, -0.2342],\n",
       "        [ 0.0364,  0.1461, -0.2013, -0.2400, -0.3631,  0.3954,  0.4890, -0.2404],\n",
       "        [ 0.2429, -0.1025, -0.5825,  0.1807, -0.2435,  0.3893,  0.1379,  0.0524],\n",
       "        [ 0.2124, -0.0534, -0.1428, -0.2280, -0.4336,  0.0330,  0.4035,  0.2857],\n",
       "        [ 0.1100,  0.3438,  0.1894, -0.4629, -0.3680, -0.2646,  0.2554, -0.0948],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_encoder_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1182,  0.5731,  0.2595, -0.5787, -0.4998, -0.0218,  0.4067, -0.2413],\n",
       "        [-0.0100,  0.2469, -0.0475, -0.3790, -0.3101,  0.1612,  0.1219, -0.2342],\n",
       "        [ 0.0364,  0.1461, -0.2013, -0.2400, -0.3631,  0.3954,  0.4890, -0.2404],\n",
       "        [ 0.2429, -0.1025, -0.5825,  0.1807, -0.2435,  0.3893,  0.1379,  0.0524],\n",
       "        [ 0.2124, -0.0534, -0.1428, -0.2280, -0.4336,  0.0330,  0.4035,  0.2857],\n",
       "        [ 0.1100,  0.3438,  0.1894, -0.4629, -0.3680, -0.2646,  0.2554, -0.0948],\n",
       "        [ 0.0340,  0.1789,  0.0060, -0.1277,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0082,  0.0880, -0.0293, -0.0322,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0006,  0.0419, -0.0276, -0.0049,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0030,  0.0195, -0.0190,  0.0017,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9074, -2.1085, -0.3591,  1.1981, -0.3069],\n",
       "         [-0.8585, -0.1450,  1.7102,  1.5498,  0.8394],\n",
       "         [ 0.6814,  1.2009,  2.8156, -0.8746,  0.4696],\n",
       "         [-0.8103,  0.8221, -0.8722, -1.5284, -0.3509],\n",
       "         [-0.6266, -0.0082, -1.1014,  0.9016,  0.5329],\n",
       "         [-1.1077, -0.7007,  2.5961,  0.6664,  0.8542],\n",
       "         [ 0.5068, -1.8874, -0.2909, -0.1333,  1.1500],\n",
       "         [ 0.1781,  0.2571,  1.3376,  0.9025,  0.5470],\n",
       "         [-0.6205, -1.5771,  0.1747, -0.3322, -0.2763],\n",
       "         [ 0.2494, -0.2989, -0.3181,  1.2363,  0.7480]],\n",
       "\n",
       "        [[ 0.9074, -2.1085, -0.3591,  1.1981, -0.3069],\n",
       "         [ 0.7964, -0.1401,  0.6662, -0.4248,  0.1476],\n",
       "         [-0.3629, -0.9660,  0.8563,  0.7797, -1.0582],\n",
       "         [-1.1474,  1.1108,  0.7414, -0.3431, -0.3324],\n",
       "         [-0.6205, -1.5771,  0.1747, -0.3322, -0.2763],\n",
       "         [ 0.2494, -0.2989, -0.3181,  1.2363,  0.7480],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_emb_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Use the output of encoder as encoder state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_state = encoder_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 Use the last hidden output as the initial hidden state for the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.decoding_size = 2 * args.rnn_hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_hidden = encoder_h.permute(1,0,2).contiguous().view(args.batch_size,-1)\n",
    "initial_hidden.shape "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "* 2*hidden_size of encoder = hidden_size of decoder\n",
    "* 8 will be the hidden_size of the decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 5A pass the initial hidden state to linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_fc = nn.Linear(args.decoding_size,args.decoding_size)\n",
    "initial_hidden =  hidden_fc(initial_hidden)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Define the target embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(15, 5, padding_idx=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embedding = nn.Embedding(embedding_dim=args.target_embedding_size,\n",
    "                                num_embeddings=args.target_vocab_size,\n",
    "                                padding_idx=args.padding_idx)\n",
    "target_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 Define the decoder cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_cell = nn.GRUCell(input_size=args.target_embedding_size + args.decoding_size, # for context vector\n",
    "                          hidden_size=args.decoding_size,\n",
    "                          bias=False,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7A Define the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Linear(2*args.decoding_size,args.target_vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 Initial the context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = torch.zeros(size=(args.batch_size,args.decoding_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9 Get the input of the target sequence [batch,max_seq_length_target_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  6,  9,  8, 12,  5, 10, 13,  4],\n",
       "        [ 2,  7, 11, 14,  4,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_input = sample[\"target_x_vector\"]\n",
    "target_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_input.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 Permute to [Seq,Batch]\n",
    "\n",
    "So we can loop through data directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_input = target_input.permute(1,0)\n",
    "target_input.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11 loop through seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embedding(target_input[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t = initial_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mechanism(encoder_state,query_vector):\n",
    "    batch_size,seq_size,emb_size = encoder_state.size()\n",
    "    \n",
    "    vector_score = torch.sum(encoder_state * query_vector.view(batch_size,1,emb_size),\n",
    "                             dim=2)\n",
    "    \n",
    "    vector_prob = F.softmax(vector_score,dim=1)\n",
    "    \n",
    "    weight_vector = encoder_state * vector_prob.view(batch_size,seq_size,1)\n",
    "    \n",
    "    context_vector = torch.sum(weight_vector,dim=1)\n",
    "    \n",
    "    return context_vector,vector_prob,vector_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vector = []\n",
    "for xt in target_input:\n",
    "    # step 11a find the embedding of input\n",
    "    xt_vector = target_embedding(xt)\n",
    "    # step 11b combine the embed vector and context vector which the input to decoder cell\n",
    "    cell_input = torch.cat((xt_vector,context_vector),dim=1) \n",
    "    # shape [batch ,target_embedding_size + decoding_size]\n",
    "    h_t = decoder_cell(cell_input,h_t)\n",
    "    \n",
    "    # attention mechanism\n",
    "    context_vector,p_attn,_ = attention_mechanism(encoder_state=encoder_state,\n",
    "                                                  query_vector=h_t)\n",
    "    \n",
    "    pred_vector = torch.cat((context_vector,h_t),dim=1)\n",
    "    score_out_index = classifier(pred_vector)\n",
    "    output_vector.append(score_out_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 15])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_out = torch.stack(output_vector).permute(1,0,2)\n",
    "decode_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
