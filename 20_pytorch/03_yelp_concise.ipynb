{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from functools import partial\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torchdata.datapipes as dp\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "from torchtext import vocab\n",
    "from torch.utils.data.backward_compatibility import worker_init_fn\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    default_collate)\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# manipulation\n",
    "import numpy as np\n",
    "\n",
    "# other\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\models\\yelp\\model.pth\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # data and file paths\n",
    "    url = \"https://drive.google.com/u/0/uc?id=1Lmv4rsJiCWVs1nzs4ywA9YI-ADsTf6WB&export=download\",\n",
    "    download_data = False,\n",
    "    csv_path = \"../data/reviews_with_splits_lite.csv\",\n",
    "    \n",
    "    ## vocab\n",
    "    frequency_cutoff = 25,\n",
    "    special_tkn = \"<unk>\",\n",
    "    \n",
    "    ## save\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir = \"../models/yelp/\",\n",
    "    \n",
    "    # training\n",
    "    batch_size = 128,\n",
    "    learning_rate=0.001,\n",
    "    early_stopping_criteria = 5,\n",
    "    num_epochs = 100,\n",
    "    \n",
    "    # runtime\n",
    "    seed = 42,\n",
    "    cuda = torch.cuda.is_available,\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    expand_filepaths_to_save_dir = True\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    if not Path(args.save_dir).exists():\n",
    "        Path(args.save_dir).mkdir(parents=True,exist_ok=True)\n",
    "    args.model_state_file = Path(args.save_dir) / args.model_state_file\n",
    "    print(args.model_state_file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fn(split,row):\n",
    "    return row[2] == split\n",
    "\n",
    "def split_classifier_fn(row):\n",
    "    num = None\n",
    "    if row[2] == \"train\":\n",
    "        num = 0\n",
    "    elif row[2] == \"test\":\n",
    "        num = 2\n",
    "    elif row[2] == \"val\":\n",
    "        num = 1 \n",
    "    return num\n",
    "\n",
    "\n",
    "def review_token_fn(row):\n",
    "    return [token \n",
    "            for token in row[1].split()\n",
    "            if token not in string.punctuation]\n",
    "        \n",
    "def rating_token_fn(row):\n",
    "    return [row[0]]\n",
    "\n",
    "def create_dataset(review_vocab,rating_vocab,row):\n",
    "    review_vector = np.zeros(shape=len(review_vocab),dtype=np.float32)\n",
    "    review_vector[review_vocab.lookup_indices(review_token_fn(row))] = 1\n",
    "    \n",
    "    target = rating_vocab.lookup_indices([row[0]])[-1]\n",
    "    \n",
    "    return {\"x\":review_vector,\n",
    "            \"y\":target}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_and_build_dataset(args):\n",
    "    \n",
    "    # dict for return\n",
    "    vocab_dict = defaultdict()\n",
    "    dataset_dict = defaultdict()\n",
    "    \n",
    "    if args.download_data:\n",
    "        stream = dp.iter.GDriveReader(dp.iter.IterableWrapper([args.url]))\n",
    "    else:\n",
    "        stream = dp.iter.FileOpener(dp.iter.IterableWrapper([args.csv_path]))\n",
    "    row = stream.parse_csv(skip_lines=1)\n",
    "    train_set,val_set,test_set = row.demux(num_instances=3,classifier_fn=split_classifier_fn,\n",
    "                                           buffer_size=100000)\n",
    "    \n",
    "    # creating the review vocab\n",
    "    review_token =  train_set.map(review_token_fn)\n",
    "    review_vocab = vocab.build_vocab_from_iterator(review_token,min_freq=args.frequency_cutoff,\n",
    "                                                   specials=[args.special_tkn])\n",
    "    review_vocab.set_default_index(review_vocab[args.special_tkn])\n",
    "    \n",
    "    # creating the rating vocab\n",
    "    rating_token = train_set.map(rating_token_fn)\n",
    "    rating_vocab = vocab.build_vocab_from_iterator(rating_token)\n",
    "    \n",
    "    train_set = train_set.shuffle().sharding_filter()\n",
    "    train_dataset = train_set.map(partial(create_dataset,review_vocab,rating_vocab))\n",
    "    train_dataset = train_dataset.batch(batch_size=args.batch_size,drop_last=True)\n",
    "    \n",
    "    val_dataset = val_set.map(partial(create_dataset,review_vocab,rating_vocab))\n",
    "    val_dataset = val_dataset.batch(batch_size=args.batch_size,drop_last=True)\n",
    "    \n",
    "    \n",
    "    test_dataset = test_set.map(partial(create_dataset,review_vocab,rating_vocab))\n",
    "    test_dataset = test_dataset.batch(batch_size=args.batch_size,drop_last=True)\n",
    "    \n",
    "    vocab_dict[\"review\"] = review_vocab\n",
    "    vocab_dict[\"rating\"] = rating_vocab\n",
    "    \n",
    "    dataset_dict[\"train\"] = train_dataset\n",
    "    dataset_dict[\"val\"] = val_dataset\n",
    "    dataset_dict[\"test\"] = test_dataset\n",
    "    \n",
    "    return vocab_dict , dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:262: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "vocab_dict , dataset_dict = build_vocab_and_build_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset,batch_size,shuffle=False,device=\"cpu\",\n",
    "                     worker_init_fn=worker_init_fn):\n",
    "    dataloader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,\n",
    "                            worker_init_fn=worker_init_fn,\n",
    "                            collate_fn=lambda x: {k:v.to(device).float() \n",
    "                                                      for x_ in default_collate(x)\n",
    "                                                      for k,v in x_.items()})\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = generate_batches(dataset=dataset_dict[\"train\"],\n",
    "                               batch_size=args.batch_size,\n",
    "                               shuffle=True,device=args.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self,num_features) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features,\n",
    "                             out_features=1)\n",
    "    \n",
    "    def forward(self,input,apply_sigmoid=False):\n",
    "        y_out = self.fc1(input).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = F.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training routine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting and helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {\"stop_early\":False,\n",
    "            \"early_stopping_step\":0,\n",
    "            \"early_stopping_best_val\":1e8,\n",
    "            \"learning_rate\":args.learning_rate,\n",
    "            \"model_filename\":args.model_state_file,\n",
    "            \"epoch_index\":0,\n",
    "            \"train_loss\":[],\n",
    "            \"train_acc\":[],\n",
    "            \"val_loss\":[],\n",
    "            \"val_acc\":[],\n",
    "            \"test_loss\":[],\n",
    "            \"test_acc\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_train_state(train_state,args,model):\n",
    "    # save one model at least\n",
    "    if train_state[\"epoch_index\"] == 0:\n",
    "        torch.save(model.state_dict,train_state[\"model_filename\"])\n",
    "        train_state[\"stop_early\"] = False\n",
    "    \n",
    "    # save model if it performed\n",
    "    elif train_state[\"epoch_index\"] >= 1:\n",
    "        loss_tm1,loss_t = train_state[\"val_loss\"][-2:]\n",
    "        \n",
    "        # it the loss worsen\n",
    "        if loss_t >= train_state[\"early_stopping_best_val\"]:\n",
    "            train_state[\"early_stopping_step\"] +=1\n",
    "        \n",
    "        # model is learning\n",
    "        else:\n",
    "            # save the model\n",
    "            torch.save(model.state_dict(),train_state[\"model_filename\"])\n",
    "            train_state[\"early_stopping_step\"] = 0\n",
    "        \n",
    "        train_state[\"stop_early\"] = train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
    "        \n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed,cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)    \n",
    "        \n",
    "set_seed_everywhere(args.seed,args.cuda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss,optimizer,accuracy,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ReviewClassifier(len(vocab_dict[\"review\"])).to(args.device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "acc_fn = Accuracy(task=\"binary\",num_classes=2).to(args.device)\n",
    "optimizer = optim.Adam(params=classifier.parameters(),\n",
    "                       lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode=\"min\",\n",
    "                                                 factor=0.5,patience=1)\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 2/100 [00:24<20:07, 12.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\DataspellProjects\\NLP\\20_pytorch\\03_yelp_concise.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m running_acc \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m classifier\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_index,batch_dict \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batch_generator):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m classifier(batch_dict[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;32md:\\DataspellProjects\\NLP\\20_pytorch\\03_yelp_concise.ipynb Cell 20\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_batches\u001b[39m(dataset,batch_size,shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                      worker_init_fn\u001b[39m=\u001b[39mworker_init_fn):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     dataloader \u001b[39m=\u001b[39m DataLoader(dataset\u001b[39m=\u001b[39mdataset,batch_size\u001b[39m=\u001b[39mbatch_size,shuffle\u001b[39m=\u001b[39mshuffle,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                             worker_init_fn\u001b[39m=\u001b[39mworker_init_fn,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                             collate_fn\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: {k:v\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat() \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                                       \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m default_collate(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                                       \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m x_\u001b[39m.\u001b[39mitems()})\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39myield\u001b[39;00m batch\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:34\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[0;32m     33\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_iter))\n\u001b[0;32m     35\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:185\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[39melse\u001b[39;00m:  \u001b[39m# Decided against using `contextlib.nullcontext` for performance reasons\u001b[39;00m\n\u001b[0;32m    184\u001b[0m             _check_iterator_valid(datapipe, iterator_id)\n\u001b[1;32m--> 185\u001b[0m             response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(request)\n\u001b[0;32m    186\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\datapipes\\datapipe.py:351\u001b[0m, in \u001b[0;36m_IterDataPipeSerializationWrapper.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 351\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_datapipe\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:185\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[39melse\u001b[39;00m:  \u001b[39m# Decided against using `contextlib.nullcontext` for performance reasons\u001b[39;00m\n\u001b[0;32m    184\u001b[0m             _check_iterator_valid(datapipe, iterator_id)\n\u001b[1;32m--> 185\u001b[0m             response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(request)\n\u001b[0;32m    186\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\grouping.py:95\u001b[0m, in \u001b[0;36mBatcherIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[DataChunk]:\n\u001b[0;32m     94\u001b[0m     batch: List \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 95\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe:\n\u001b[0;32m     96\u001b[0m         batch\u001b[39m.\u001b[39mappend(x)\n\u001b[0;32m     97\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:185\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[39melse\u001b[39;00m:  \u001b[39m# Decided against using `contextlib.nullcontext` for performance reasons\u001b[39;00m\n\u001b[0;32m    184\u001b[0m             _check_iterator_valid(datapipe, iterator_id)\n\u001b[1;32m--> 185\u001b[0m             response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(request)\n\u001b[0;32m    186\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:123\u001b[0m, in \u001b[0;36mMapperIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[T_co]:\n\u001b[0;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe:\n\u001b[1;32m--> 123\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:88\u001b[0m, in \u001b[0;36mMapperIterDataPipe._apply_fn\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply_fn\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_col \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_col \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(data)\n\u001b[0;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_col \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m         res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(data)\n",
      "\u001b[1;32md:\\DataspellProjects\\NLP\\20_pytorch\\03_yelp_concise.ipynb Cell 20\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_dataset\u001b[39m(review_vocab,rating_vocab,row):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     review_vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros(shape\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(review_vocab),dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     review_vector[review_vocab\u001b[39m.\u001b[39mlookup_indices(review_token_fn(row))] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataspellProjects/NLP/20_pytorch/03_yelp_concise.ipynb#Y110sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     target \u001b[39m=\u001b[39m rating_vocab\u001b[39m.\u001b[39mlookup_indices([row[\u001b[39m0\u001b[39m]])[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_index in tqdm(range(args.num_epochs)):\n",
    "    train_state[\"epoch_index\"] = epoch_index\n",
    "    \n",
    "    # train step\n",
    "    batch_generator = generate_batches(dataset_dict[\"train\"],\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       shuffle=True,\n",
    "                                       device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch_index,batch_dict in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = classifier(batch_dict[\"x\"])\n",
    "        \n",
    "        loss = loss_fn(y_pred,batch_dict[\"y\"])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t -running_loss) / (batch_index +1)\n",
    "        \n",
    "        acc = acc_fn(y_pred,batch_dict[\"y\"])\n",
    "        acc_t = acc.item()\n",
    "        running_acc += (acc_t - running_acc) / (batch_index+1)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_state[\"train_loss\"].append(running_loss)\n",
    "    train_state[\"train_acc\"].append(running_acc)\n",
    "    \n",
    "    \n",
    "    # val step\n",
    "    batch_generator = generate_batches(dataset=dataset_dict[\"val\"],\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       shuffle=False,\n",
    "                                       device=args.device)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.eval()\n",
    "    \n",
    "    for batch_index,batch_dict in enumerate(batch_generator):\n",
    "        with torch.inference_mode():\n",
    "            y_pred  = classifier(batch_dict[\"x\"])\n",
    "            \n",
    "            loss = loss_fn(y_pred,batch_dict[\"y\"])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) /(batch_index+1)\n",
    "            \n",
    "            acc = acc_fn(y_pred,batch_dict[\"y\"])\n",
    "            acc_t = acc.item()\n",
    "            running_acc += (acc_t - running_acc) / (batch_index+1)\n",
    "    \n",
    "    train_state[\"val_loss\"].append(running_loss)\n",
    "    train_state[\"val_acc\"].append(running_acc)\n",
    "       \n",
    "    train_state = update_train_state(train_state,args,classifier)\n",
    "    scheduler.step(train_state[\"val_loss\"][-1])\n",
    "    \n",
    "    if train_state[\"stop_early\"]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
