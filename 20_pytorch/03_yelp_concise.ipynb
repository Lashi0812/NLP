{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2uCFZNQ19es"
      },
      "source": [
        "# Basic Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Pfnk1nKP19eu"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "import torch\n",
        "\n",
        "args = Namespace(\n",
        "    colab = False,\n",
        "    # data and file paths\n",
        "    url = \"https://drive.google.com/u/0/uc?id=1Lmv4rsJiCWVs1nzs4ywA9YI-ADsTf6WB&export=download\",\n",
        "    csv_path = \"../data/reviews_with_splits_lite.csv\",\n",
        "    \n",
        "    ## vocab\n",
        "    frequency_cutoff = 25,\n",
        "    special_tkn = \"<unk>\",\n",
        "    \n",
        "    ## save\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir = \"../models/yelp/\",\n",
        "    \n",
        "    # training\n",
        "    batch_size = 128,\n",
        "    learning_rate=0.001,\n",
        "    early_stopping_criteria = 5,\n",
        "    num_epochs = 100,\n",
        "    \n",
        "    # runtime\n",
        "    seed = 42,\n",
        "    cuda = torch.cuda.is_available,\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    expand_filepaths_to_save_dir = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iZrvV2gC19er"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import requests\n",
        "from functools import partial\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data.backward_compatibility import worker_init_fn\n",
        "from torch.utils.data import (\n",
        "    DataLoader,\n",
        "    default_collate)\n",
        "\n",
        "from torchtext import vocab\n",
        "try:\n",
        "    import torchdata.datapipes as dp\n",
        "except:\n",
        "    !pip install torchdata\n",
        "    import torchdata.datapipes as dp\n",
        "try:\n",
        "    from torchmetrics import Accuracy\n",
        "except:\n",
        "    !pip install torchmetrics\n",
        "    from torchmetrics import Accuracy\n",
        "\n",
        "# manipulation\n",
        "import numpy as np\n",
        "\n",
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# other\n",
        "from tqdm import tqdm\n",
        "\n",
        "try :\n",
        "    import gdown\n",
        "    from google.colab import drive\n",
        "    args.colab = True\n",
        "\n",
        "    # Paths\n",
        "    DRIVE_PATH = Path(\"/content/drive\")\n",
        "    MODEL_SAVE_PATH = Path(\"/content/drive/Othercomputers/My PC/drive/models/yelp\")\n",
        "    args.save_dir = MODEL_SAVE_PATH\n",
        "    Path(\"data\").mkdir(parents=True,exist_ok=True)\n",
        "    args.csv_path = \"data/reviews_with_splits_lite.csv\"\n",
        "\n",
        "    # mount drive\n",
        "    drive.mount(str(DRIVE_PATH))\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args.colab"
      ],
      "metadata": {
        "id": "o_lBWvCfIjix",
        "outputId": "ab5303e8-0cdc-4ba8-b1a6-92f2ad7a85d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.colab:\n",
        "    gdown.download(args.url,args.csv_path)\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    if not Path(args.save_dir).exists():\n",
        "        Path(args.save_dir).mkdir(parents=True,exist_ok=True)\n",
        "    args.model_state_file = Path(args.save_dir) / args.model_state_file\n",
        "    print(args.model_state_file) "
      ],
      "metadata": {
        "id": "OlRw8Rrh-VO0",
        "outputId": "2b592c0b-f61a-4570-8b08-07436a795682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1Lmv4rsJiCWVs1nzs4ywA9YI-ADsTf6WB&export=download\n",
            "To: /content/data/reviews_with_splits_lite.csv\n",
            "100%|██████████| 39.9M/39.9M [00:01<00:00, 24.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Othercomputers/My PC/drive/models/yelp/model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYZ0Fq2u19ew"
      },
      "source": [
        "# Data Pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nJ2vaNCN19ew"
      },
      "outputs": [],
      "source": [
        "def filter_fn(split,row):\n",
        "    return row[2] == split\n",
        "\n",
        "def split_classifier_fn(row):\n",
        "    num = None\n",
        "    if row[2] == \"train\":\n",
        "        num = 0\n",
        "    elif row[2] == \"test\":\n",
        "        num = 2\n",
        "    elif row[2] == \"val\":\n",
        "        num = 1 \n",
        "    return num\n",
        "\n",
        "\n",
        "def review_token_fn(row):\n",
        "    return [token \n",
        "            for token in row[1].split()\n",
        "            if token not in string.punctuation]\n",
        "        \n",
        "def rating_token_fn(row):\n",
        "    return [row[0]]\n",
        "\n",
        "def create_dataset(review_vocab,rating_vocab,row):\n",
        "    review_vector = np.zeros(shape=len(review_vocab),dtype=np.float32)\n",
        "    review_vector[review_vocab.lookup_indices(review_token_fn(row))] = 1\n",
        "    \n",
        "    target = rating_vocab.lookup_indices([row[0]])[-1]\n",
        "    \n",
        "    return {\"x\":review_vector,\n",
        "            \"y\":target}\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args.colab"
      ],
      "metadata": {
        "id": "5JLK4IK5IAOn",
        "outputId": "8120451d-785a-4aca-e03d-c2df0bde8a04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QI2SCN4M19ex"
      },
      "outputs": [],
      "source": [
        "def build_vocab_and_build_dataset(args):\n",
        "    \n",
        "    # dict for return\n",
        "    vocab_dict = defaultdict()\n",
        "    dataset_dict = defaultdict()\n",
        "    \n",
        "    stream = dp.iter.FileOpener(dp.iter.IterableWrapper([args.csv_path]))\n",
        "    row = stream.parse_csv(skip_lines=1)\n",
        "    train_set,val_set,test_set = row.demux(num_instances=3,classifier_fn=split_classifier_fn,\n",
        "                                           buffer_size=100000)\n",
        "    \n",
        "    # creating the review vocab\n",
        "    review_token =  train_set.map(review_token_fn)\n",
        "    review_vocab = vocab.build_vocab_from_iterator(review_token,min_freq=args.frequency_cutoff,\n",
        "                                                   specials=[args.special_tkn])\n",
        "    review_vocab.set_default_index(review_vocab[args.special_tkn])\n",
        "    \n",
        "    # creating the rating vocab\n",
        "    rating_token = train_set.map(rating_token_fn)\n",
        "    rating_vocab = vocab.build_vocab_from_iterator(rating_token)\n",
        "    \n",
        "    train_set = train_set.shuffle().sharding_filter()\n",
        "    train_dataset = train_set.map(partial(create_dataset,review_vocab,rating_vocab))\n",
        "    train_dataset = train_dataset.batch(batch_size=args.batch_size,drop_last=True)\n",
        "    \n",
        "    val_dataset = val_set.map(partial(create_dataset,review_vocab,rating_vocab))\n",
        "    val_dataset = val_dataset.batch(batch_size=args.batch_size,drop_last=True)\n",
        "    \n",
        "    \n",
        "    test_dataset = test_set.map(partial(create_dataset,review_vocab,rating_vocab))\n",
        "    test_dataset = test_dataset.batch(batch_size=args.batch_size,drop_last=True)\n",
        "    \n",
        "    vocab_dict[\"review\"] = review_vocab\n",
        "    vocab_dict[\"rating\"] = rating_vocab\n",
        "    \n",
        "    dataset_dict[\"train\"] = train_dataset\n",
        "    dataset_dict[\"val\"] = val_dataset\n",
        "    dataset_dict[\"test\"] = test_dataset\n",
        "    \n",
        "    return vocab_dict , dataset_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mcJzp27j19ey",
        "outputId": "9315a63d-5472-4f59-8d5e-6b5f6a263e88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/datapipes/iter/combining.py:262: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ],
      "source": [
        "vocab_dict , dataset_dict = build_vocab_and_build_dataset(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cO0L7db519ey"
      },
      "outputs": [],
      "source": [
        "def generate_batches(dataset,batch_size,shuffle=False,device=\"cpu\",\n",
        "                     worker_init_fn=worker_init_fn):\n",
        "    dataloader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,\n",
        "                            worker_init_fn=worker_init_fn,\n",
        "                            collate_fn=lambda x: {k:v.to(device).float() \n",
        "                                                      for x_ in default_collate(x)\n",
        "                                                      for k,v in x_.items()})\n",
        "    \n",
        "    for batch in dataloader:\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IExKzYYL19ez"
      },
      "outputs": [],
      "source": [
        "train_batch = generate_batches(dataset=dataset_dict[\"train\"],\n",
        "                               batch_size=args.batch_size,\n",
        "                               shuffle=True,device=args.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaQgxsrT19e0"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1OGA7QAG19e0"
      },
      "outputs": [],
      "source": [
        "class ReviewClassifier(nn.Module):\n",
        "    def __init__(self,num_features) -> None:\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features=num_features,\n",
        "                             out_features=1)\n",
        "    \n",
        "    def forward(self,input,apply_sigmoid=False):\n",
        "        y_out = self.fc1(input).squeeze()\n",
        "        if apply_sigmoid:\n",
        "            y_out = F.sigmoid(y_out)\n",
        "        return y_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86e-yOCI19e1"
      },
      "source": [
        "# Training routine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qB71Oea19e1"
      },
      "source": [
        "## setting and helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R0z609oV19e1"
      },
      "outputs": [],
      "source": [
        "def make_train_state(args):\n",
        "    return {\"stop_early\":False,\n",
        "            \"early_stopping_step\":0,\n",
        "            \"early_stopping_best_val\":1e8,\n",
        "            \"learning_rate\":args.learning_rate,\n",
        "            \"model_filename\":args.model_state_file,\n",
        "            \"epoch_index\":0,\n",
        "            \"train_loss\":[],\n",
        "            \"train_acc\":[],\n",
        "            \"val_loss\":[],\n",
        "            \"val_acc\":[],\n",
        "            \"test_loss\":-1,\n",
        "            \"test_acc\":-1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RO76TwON19e1"
      },
      "outputs": [],
      "source": [
        "def update_train_state(train_state,args,model):\n",
        "    # save one model at least\n",
        "    if train_state[\"epoch_index\"] == 0:\n",
        "        torch.save(model.state_dict,train_state[\"model_filename\"])\n",
        "        train_state[\"stop_early\"] = False\n",
        "    \n",
        "    # save model if it performed\n",
        "    elif train_state[\"epoch_index\"] >= 1:\n",
        "        loss_tm1,loss_t = train_state[\"val_loss\"][-2:]\n",
        "        \n",
        "        # it the loss worsen\n",
        "        if loss_t >= train_state[\"early_stopping_best_val\"]:\n",
        "            train_state[\"early_stopping_step\"] +=1\n",
        "        \n",
        "        # model is learning\n",
        "        else:\n",
        "            # save the model\n",
        "            torch.save(model.state_dict(),train_state[\"model_filename\"])\n",
        "            train_state[\"early_stopping_step\"] = 0\n",
        "        \n",
        "        train_state[\"stop_early\"] = train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
        "        \n",
        "    return train_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eLqSw3aR19e2"
      },
      "outputs": [],
      "source": [
        "def set_seed_everywhere(seed,cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)    \n",
        "        \n",
        "set_seed_everywhere(args.seed,args.cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMXgCa8019e2"
      },
      "source": [
        "## loss,optimizer,accuracy,model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ad37F-qf19e2"
      },
      "outputs": [],
      "source": [
        "classifier = ReviewClassifier(len(vocab_dict[\"review\"])).to(args.device)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "acc_fn = Accuracy(task=\"binary\",num_classes=2).to(args.device)\n",
        "optimizer = optim.Adam(params=classifier.parameters(),\n",
        "                       lr=args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode=\"min\",\n",
        "                                                 factor=0.5,patience=1)\n",
        "train_state = make_train_state(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kcfBr_L19e2"
      },
      "source": [
        "## training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3J4VkBQ19e3",
        "outputId": "8d353f01-73d9-4ada-bb9f-0c2695a3cf0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 16/100 [01:40<08:19,  5.95s/it]"
          ]
        }
      ],
      "source": [
        "for epoch_index in tqdm(range(args.num_epochs)):\n",
        "    train_state[\"epoch_index\"] = epoch_index\n",
        "    \n",
        "    # train step\n",
        "    batch_generator = generate_batches(dataset_dict[\"train\"],\n",
        "                                       batch_size=args.batch_size,\n",
        "                                       shuffle=True,\n",
        "                                       device=args.device)\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    classifier.train()\n",
        "    \n",
        "    for batch_index,batch_dict in enumerate(batch_generator):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = classifier(batch_dict[\"x\"])\n",
        "        \n",
        "        loss = loss_fn(y_pred,batch_dict[\"y\"])\n",
        "        loss_t = loss.item()\n",
        "        running_loss += (loss_t -running_loss) / (batch_index +1)\n",
        "        \n",
        "        acc = acc_fn(y_pred,batch_dict[\"y\"])\n",
        "        acc_t = acc.item()\n",
        "        running_acc += (acc_t - running_acc) / (batch_index+1)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    train_state[\"train_loss\"].append(running_loss)\n",
        "    train_state[\"train_acc\"].append(running_acc)\n",
        "    \n",
        "    \n",
        "    # val step\n",
        "    batch_generator = generate_batches(dataset=dataset_dict[\"val\"],\n",
        "                                       batch_size=args.batch_size,\n",
        "                                       shuffle=False,\n",
        "                                       device=args.device)\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    classifier.eval()\n",
        "    \n",
        "    for batch_index,batch_dict in enumerate(batch_generator):\n",
        "        with torch.inference_mode():\n",
        "            y_pred  = classifier(batch_dict[\"x\"])\n",
        "            \n",
        "            loss = loss_fn(y_pred,batch_dict[\"y\"])\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) /(batch_index+1)\n",
        "            \n",
        "            acc = acc_fn(y_pred,batch_dict[\"y\"])\n",
        "            acc_t = acc.item()\n",
        "            running_acc += (acc_t - running_acc) / (batch_index+1)\n",
        "    \n",
        "    train_state[\"val_loss\"].append(running_loss)\n",
        "    train_state[\"val_acc\"].append(running_acc)\n",
        "       \n",
        "    train_state = update_train_state(train_state,args,classifier)\n",
        "    scheduler.step(train_state[\"val_loss\"][-1])\n",
        "    \n",
        "    if train_state[\"stop_early\"]:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(args.num_epochs),train_state[\"train_loss\"])\n",
        "plt.plot(range(args.num_epochs),train_state[\"train_acc\"])\n",
        "plt.plot(range(args.num_epochs),train_state[\"val_loss\"])\n",
        "plt.plot(range(args.num_epochs),train_state[\"val_acc\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HuJAnEkVFLbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFZViSmp19e3"
      },
      "outputs": [],
      "source": [
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "classifier = classifier.to(args.device)\n",
        "\n",
        "\n",
        "batch_generator = generate_batches(dataset_dict[\"test\"], \n",
        "                                   batch_size=args.batch_size, \n",
        "                                   device=args.device,\n",
        "                                   shuffle=False)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred = classifier(x_in=batch_dict['x'])\n",
        "\n",
        "    # compute the loss\n",
        "    loss = loss_fn(y_pred, batch_dict['y'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = acc_fn(y_pred, batch_dict['y'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}