{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# built in\n",
    "from argparse import Namespace\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.utils.data.backward_compatibility import worker_init_fn\n",
    "## others\n",
    "from torchdata import datapipes as dp\n",
    "from torchtext import vocab\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# manipulation\n",
    "import numpy as np\n",
    "\n",
    "# others\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # data\n",
    "    data_base_path = \"../data/cbow/\",\n",
    "    datasets = [\"train\",\"val\",\"test\"],\n",
    "    delimiter = \"#\",\n",
    "    \n",
    "    # vocab\n",
    "    mask_tkn = \"<MASK>\",\n",
    "    unk_tkn = \"<UKN>\",\n",
    "    window_size = 2,\n",
    "    \n",
    "    # model\n",
    "    embedding_dim = 50,\n",
    "    model_base_path = \"../models/cbow/\",\n",
    "    model_filename = \"model.pth\",\n",
    "    \n",
    "    \n",
    "    # training\n",
    "    batches = 32,\n",
    "    learning_rate = 0.001,\n",
    "    num_epochs = 100,\n",
    "    early_stopping_criteria = 5,\n",
    "    \n",
    "    # running options\n",
    "    cuda = torch.cuda.is_available(),\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    seed = 1432\n",
    ")\n",
    "\n",
    "for k,v in args._get_kwargs():\n",
    "    if \"base\" in k:\n",
    "        Path(v).mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapipes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open and parse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_tuples(row):\n",
    "    return (row[0].split(\" \"),[row[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def open_parsed_dict(args):\n",
    "    csv_pipe_dict = {}\n",
    "    for fname in args.datasets:\n",
    "        csv_pipe_dict[fname] = dp.iter\\\n",
    "            .FileOpener([args.data_base_path+f\"{fname}.csv\"])\\\n",
    "            .parse_csv(delimiter=args.delimiter)\\\n",
    "            .map(convert_to_tuples)\n",
    "    return csv_pipe_dict            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "csv_parse_dict = open_parsed_dict(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['start', 'of', 'project', 'gutenberg'], ['the'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(csv_parse_dict[\"train\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def join_context_target_fn(row):\n",
    "    return row[0]+row[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab(train_pipe,args=args):\n",
    "    combined_dp = train_pipe.map(join_context_target_fn)\n",
    "    cbow_vocab = vocab.build_vocab_from_iterator(combined_dp,\n",
    "                                                 specials=[args.unk_tkn,args.mask_tkn])\n",
    "    cbow_vocab.set_default_index(cbow_vocab[args.unk_tkn])\n",
    "    return cbow_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "cbow_vocab = build_vocab(csv_parse_dict[\"train\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6938, 5, 5181, 4859]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_vocab.lookup_indices(['start', 'of', 'project', 'gutenberg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_vocab.lookup_indices(['the'])[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize(context,vocab,args):\n",
    "    indices = vocab.lookup_indices(context)\n",
    "    vector = np.zeros(args.window_size*2,dtype=np.float32)\n",
    "    vector[:len(indices)] = indices\n",
    "    vector[len(indices):] = vocab[args.mask_tkn]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset_dict(vocab,args,row):\n",
    "    context_vector = vectorize(row[0],vocab=vocab,args=args)\n",
    "    target_index = vocab.lookup_indices(row[1])[-1]\n",
    "    return {\"x\":context_vector,\n",
    "            \"y\":target_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def build_dataset_dict(csv_parse_dict,vocab,args=args):\n",
    "    dataset_dict = {}\n",
    "    fn = partial(create_dataset_dict,vocab,args)\n",
    "    for dataset_name,csv_parse_pipe in csv_parse_dict.items():\n",
    "        if dataset_name == \"train\":\n",
    "            csv_parse_pipe = csv_parse_pipe.shuffle()\n",
    "        \n",
    "        dataset_dict[dataset_name] = csv_parse_pipe.map(fn).batch(args.batches)\n",
    "    \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': BatcherIterDataPipe,\n",
       " 'val': BatcherIterDataPipe,\n",
       " 'test': BatcherIterDataPipe}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = build_dataset_dict(csv_parse_dict,cbow_vocab)\n",
    "dataset_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(args,x):\n",
    "    return {k:v.to(args.device).long()\n",
    "            for x_ in default_collate(x)\n",
    "            for k,v in x_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batches(dataset,args,shuffle):\n",
    "    dataloader = DataLoader(dataset=dataset,batch_size=args.batches,\n",
    "                            shuffle=shuffle,drop_last=True,\n",
    "                            collate_fn=partial(collate_fn,args),\n",
    "                            worker_init_fn=worker_init_fn)\n",
    "    for batch in dataloader:\n",
    "        yield batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "class CBOWClassifier(nn.Module):\n",
    "    def __init__(self,vocabulary_size,embedding_size,padding_idx=0) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                      embedding_dim=embedding_size,\n",
    "                                      padding_idx=padding_idx)\n",
    "        self.fc = nn.Linear(in_features=embedding_size,\n",
    "                            out_features=vocabulary_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,input,apply_softmax=False):\n",
    "        embed = self.embedding(input)\n",
    "        embed_sum = F.dropout(embed.sum(dim=1),0.3)\n",
    "        out = self.fc(embed_sum)\n",
    "        if apply_softmax:\n",
    "            out = torch.softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = CBOWClassifier(vocabulary_size=len(cbow_vocab),\n",
    "                            embedding_size=args.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(generate_batches(dataset_dict[\"train\"],\n",
    "                                    args=args,shuffle=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[6175,    5,  492,    5]]), 'y': tensor([3])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6956])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(sample[\"x\"]).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def make_train_state(args=args):\n",
    "    return {\"stop_early\":False,\n",
    "            \"early_stopping_step\":0,\n",
    "            \"early_stopping_val\":1e8,\n",
    "            \"learning_rate\":args.learning_rate,\n",
    "            \"epoch_index\":0,\n",
    "            \"model_filename\":args.model_base_path+args.model_filename,\n",
    "            \"train_loss\":[],\n",
    "            \"train_acc\":[],\n",
    "            \"val_loss\":[],\n",
    "            \"val_acc\":[],\n",
    "            \"test_loss\":-1,\n",
    "            \"test_acc\":-1}\n",
    "    \n",
    "def update_train_state(train_state,model,args=args):\n",
    "    if train_state[\"epoch_index\"] == 0:\n",
    "        torch.save(model.state_dict(),train_state[\"model_filename\"])\n",
    "        train_state[\"stop_early\"] = False\n",
    "        \n",
    "    elif train_state[\"epoch_index\"] >= 1:\n",
    "        loss_tm1,loss_t = train_state[\"val_loss\"][-2:]\n",
    "        if loss_t >=  train_state[\"early_stopping_val\"]:\n",
    "            train_state[\"early_stopping_step\"] += 1\n",
    "        else:\n",
    "            torch.save(model.state_dict(),train_state[\"model_filename\"])\n",
    "            train_state[\"early_stopping_step\"] = 0 \n",
    "            \n",
    "        train_state[\"stop_early\"] = train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed,cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "set_seed_everywhere(args.seed,args.cuda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "classifier = CBOWClassifier(vocabulary_size=len(cbow_vocab),\n",
    "                            embedding_size=args.embedding_dim).to(args.device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "acc_fn = Accuracy(task=\"multiclass\",num_classes=len(cbow_vocab))\n",
    "optimizer = optim.Adam(params=classifier.parameters(),\n",
    "                       lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode=\"min\",factor=0.5,\n",
    "                                                 patience=1)\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:50<00:00,  6.50s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch_index in tqdm(range(args.num_epochs)):\n",
    "    train_state[\"epoch_index\"] = epoch_index\n",
    "    \n",
    "    # init the running variable\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    \n",
    "    # get the train dataloader\n",
    "    batch_generator = generate_batches(dataset=dataset_dict[\"train\"],\n",
    "                                       args=args,shuffle=True)\n",
    "    \n",
    "    # put the model in training mode\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch_idx,batch_dict in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "        logits = classifier(batch_dict[\"x\"])\n",
    "        # compute the loss per batch\n",
    "        loss = loss_fn(logits,batch_dict[\"y\"])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx+1)\n",
    "        \n",
    "        # compute the acc per batch\n",
    "        acc = acc_fn(logits,batch_dict[\"y\"])\n",
    "        acc_t = acc.item()\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx+1)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    train_state[\"train_loss\"].append(running_loss)\n",
    "    train_state[\"train_acc\"].append(running_acc)\n",
    "    \n",
    "    # iterate over the val dataset\n",
    "    \n",
    "    # init the running variable\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    \n",
    "    # get the val dataloader\n",
    "    batch_generator  = generate_batches(dataset=dataset_dict[\"val\"],\n",
    "                                        args=args,shuffle=False)\n",
    "    \n",
    "    # put the model in eval mode\n",
    "    classifier.eval()\n",
    "    \n",
    "    for batch_idx,batch_dict in enumerate(batch_generator):\n",
    "        with torch.inference_mode():\n",
    "            logits = classifier(batch_dict[\"x\"])\n",
    "            \n",
    "            # compute the loss\n",
    "            loss = loss_fn(logits,batch_dict[\"y\"])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t-running_loss) /(batch_idx+1)\n",
    "            \n",
    "            # compute the acc\n",
    "            acc = acc_fn(logits,batch_dict[\"y\"])\n",
    "            acc_t = acc.item()\n",
    "            running_acc += (acc_t-running_acc) /(batch_idx+1)\n",
    "            \n",
    "    train_state[\"val_loss\"].append(running_loss)\n",
    "    train_state[\"val_acc\"].append(running_acc)\n",
    "    \n",
    "    \n",
    "    train_state = update_train_state(train_state=train_state,\n",
    "                                     model=classifier,\n",
    "                                     args=args)\n",
    "    \n",
    "    scheduler.step(train_state[\"val_loss\"][-1])\n",
    "    if train_state[\"stop_early\"]:\n",
    "        break          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(target_word,vocab,embedding,args,n=5):\n",
    "    word_embedding = embedding[vocab.lookup_indices([target_word.lower()])[-1]]\n",
    "    distance = []\n",
    "    for word,index in vocab.get_stoi().items():\n",
    "        if word in [args.mask_tkn,args.unk_tkn,target_word.lower()]:\n",
    "            continue\n",
    "        distance.append((word,torch.dist(word_embedding,embedding[index])))\n",
    "    \n",
    "    results = sorted(distance,key=lambda x:x[1])[1:n+2]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('simpler', tensor(7.6247)),\n",
       " ('tale', tensor(7.6839)),\n",
       " ('wished', tensor(7.7530)),\n",
       " ('malignity', tensor(7.7582)),\n",
       " ('timid', tensor(7.8019)),\n",
       " ('decides', tensor(7.8025))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = classifier.embedding.weight.data\n",
    "get_closest(\"science\",cbow_vocab,embeddings,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
