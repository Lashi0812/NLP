{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default\n",
    "import string\n",
    "import json\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# torch\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader\n",
    ")\n",
    "from torch.utils.data.backward_compatibility import worker_init_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary \n",
    "\n",
    "1. Create the dictionary two dictionary that have bijection.\n",
    "    1. token to idx\n",
    "    2. idx to token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self,token_to_idx=None,add_unk=True,unk_token=\"<UNK>\") -> None:\n",
    "        \n",
    "        # creating the one dict for token to idx\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        # another dict for idx to token\n",
    "        self._idx_to_token = {idx:token \n",
    "                              for token,idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    def add_token(self,token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    def add_tokens(self,tokens):\n",
    "        return [self.add_token(token) \n",
    "                for token in tokens]\n",
    "        \n",
    "    def lookup_token(self,token):\n",
    "        \"\"\"\n",
    "        Return the index of the token\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token,self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self,index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(f\"the index {index} is not in the Vocabulary.\")\n",
    "        return self._idx_to_token(index)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"<Vocabulary(size={len(self)})>\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)     \n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls,contents):\n",
    "        return cls(**contents)  \n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {\"token_to_idx\":self._token_to_idx,\n",
    "                \"add_unk\":self._add_unk,\n",
    "                \"unk_token\":self._unk_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/reviews_with_splits_lite.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def __init__(self,review_vocab,rating_vocab) -> None:\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "    \n",
    "    def vectorize(self,review):\n",
    "        one_hot = np.zeros(shape=len(self.review_vocab),dtype=np.float32)\n",
    "        \n",
    "        for token in review:\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        \n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod \n",
    "    def from_dataframe(cls,review_df,cutoff=25):\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        #? creating the rating vocab\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "        \n",
    "        #? word the word in whole rating dataframe\n",
    "        review_vocab.word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    review_vocab.word_counts[word] += 1\n",
    "        \n",
    "        #? creating the token greater than 25\n",
    "        for word,count in sorted(review_vocab.word_counts.items(), key=lambda x: (-x[1], x[0])):\n",
    "            if count >= cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "                \n",
    "        return cls(review_vocab,rating_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls,contents):\n",
    "        review_vocab = Vocabulary.from_serializable(contents[\"review_vocab\"])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents[\"rating_vocab\"])\n",
    "        \n",
    "        return cls(review_vocab=review_vocab,rating_vocab=rating_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {\"review_vocab\":self.review_vocab.to_serializable(),\n",
    "                \"rating_vocab\":self.rating_vocab.to_serializable()}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,review_df,vectorizer) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.review_df[self.review_df.split == \"train\"]\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.review_df[self.review_df.split == \"val\"]\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.review_df[self.review_df.split == \"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self.lookup_dict = {\"train\":(self.train_df,self.train_size),\n",
    "                            \"val\":(self.val_df,self.val_size),\n",
    "                            \"test\":(self.test_df,self.test_size)}\n",
    "        \n",
    "        self.set_split(\"train\")\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        review_vector = self._vectorizer.vectorize(row.review)\n",
    "        rating_vector = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "        \n",
    "        return {\"x_data\":review_vector,\n",
    "                \"y_data\":rating_vector} \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls,review_csv):\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split == \"train\"]\n",
    "        return cls(review_df,Vectorizer.from_dataframe(train_review_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls,review_csv,vectorizer_filepath):\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(review_df,vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return Vectorizer.from_serializable(json.loads(fp))\n",
    "        \n",
    "    def save_vectorizer(self,vectorizer_filepath):\n",
    "        with open(vectorizer_filepath,\"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(json.load(fp)))\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self,split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df,self._target_size = self.lookup_dict[split]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def get_num_batches(self,batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset,batch_size,shuffle=True,\n",
    "                     drop_last=True,device=\"cpu\"):\n",
    "    dataloader = DataLoader(dataset=dataset,batch_size=batch_size,\n",
    "                            drop_last=drop_last,shuffle=shuffle,\n",
    "                            worker_init_fn=worker_init_fn)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name in data_dict.keys():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = ReviewDataset.load_dataset_and_make_vectorizer(\"../data/reviews_with_splits_lite.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([1., 0., 0., ..., 0., 0., 0.], dtype=float32), 'y_data': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = next(sample)\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7497"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.get_vectorizer().review_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_vectorizer().review_vocab.lookup_token(\"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_vectorizer().review_vocab.lookup_token(\"biological\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_vectorizer().review_vocab.word_counts[\"biological\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21439"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_vectorizer().review_vocab.word_counts[\"place\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the datapipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata import datapipes as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_open = dp.iter.FileOpener([\"../data/reviews_with_splits_lite.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv = file_open.parse_csv(skip_lines=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(parse_csv,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('negative',),\n",
       " ('terrible place to work for i just heard a story of them find a girl over her biological father coming in there who she hadn t seen in years she said hi to him which upset his wife and they left she finished the rest of her day working fine the next day when she went into work they fired over that situation . i for one and boycotting texas roadhouse because any place that could be that cruel to their staff does not deserve my business . . . yelp wants me to give them a star but i don t believe they deserve it',),\n",
       " ('train',)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fn(select,row):\n",
    "    return row[2] == select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter  = parse_csv.filter(partial(filter_fn,\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('negative',),\n",
       " ('terrible place to work for i just heard a story of them find a girl over her biological father coming in there who she hadn t seen in years she said hi to him which upset his wife and they left she finished the rest of her day working fine the next day when she went into work they fired over that situation . i for one and boycotting texas roadhouse because any place that could be that cruel to their staff does not deserve my business . . . yelp wants me to give them a star but i don t believe they deserve it',),\n",
       " ('train',)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(DataLoader(train_filter,batch_size=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review(row):\n",
    "    return row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_review = train_filter.map(get_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['terrible place to work for i just heard a story of them find a girl over her biological father coming in there who she hadn t seen in years she said hi to him which upset his wife and they left she finished the rest of her day working fine the next day when she went into work they fired over that situation . i for one and boycotting texas roadhouse because any place that could be that cruel to their staff does not deserve my business . . . yelp wants me to give them a star but i don t believe they deserve it']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(DataLoader(train_review,batch_size=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_token(review):\n",
    "    # print(\"getting token\",review,type(review))\n",
    "    return [token \n",
    "            for token in review.split(\" \")\n",
    "            if token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token = train_review.map(get_review_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_vocab = torchtext.vocab.build_vocab_from_iterator(train_token,min_freq=25,specials=[\"<unk>\"])\n",
    "review_vocab.set_default_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vocab[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vocab[\"sgg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vocab[\"story\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vocab[\"place\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7497"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vocab.lookup_token(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[993]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vocab.lookup_indices([\"story\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rating(row):\n",
    "    return [row[0]]\n",
    "train_rating = train_filter.map(get_rating)\n",
    "next(iter(train_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_vocab = torchtext.vocab.build_vocab_from_iterator(train_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method lookup_indices in module torchtext.vocab.vocab:\n",
      "\n",
      "lookup_indices(tokens: List[str]) -> List[int] method of torchtext.vocab.vocab.Vocab instance\n",
      "    Args:\n",
      "        tokens: the tokens used to lookup their corresponding `indices`.\n",
      "    \n",
      "    Returns:\n",
      "        The 'indices` associated with `tokens`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(review_vocab.lookup_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vocab.lookup_indices([\"biological\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7497"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(len(review_vocab))[review_vocab.lookup_indices(next(iter(train_token)))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(review_vocab,rating_vocab,row):\n",
    "    review_vector = np.zeros(len(review_vocab))\n",
    "    review_vector[review_vocab.lookup_indices((get_review_token(row[1])))] = 1\n",
    "    \n",
    "    rating_vector = rating_vocab.lookup_indices([row[0]])[-1]\n",
    "    \n",
    "    return {\"x_data\":review_vector,\n",
    "            \"y_data\":rating_vector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dataset = train_filter.map(partial(create_dataset,review_vocab,rating_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_iter = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_vocab.lookup_indices([\"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(sample_iter[\"x_data\"] == row[\"x_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative',\n",
       " 'terrible place to work for i just heard a story of them find a girl over her biological father coming in there who she hadn t seen in years she said hi to him which upset his wife and they left she finished the rest of her day working fine the next day when she went into work they fired over that situation . i for one and boycotting texas roadhouse because any place that could be that cruel to their staff does not deserve my business . . . yelp wants me to give them a star but i don t believe they deserve it',\n",
       " 'train']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(parse_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fn(split,row):\n",
    "    return row[2] == split\n",
    "\n",
    "def review_token_fn(row):\n",
    "    return [token\n",
    "            for token in row[1].split(\" \")\n",
    "            if token not in string.punctuation]\n",
    "\n",
    "def rating_token_fn(row):\n",
    "    return [row[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spilt(csv,split=\"train\"):\n",
    "    stream = dp.iter.FileOpener([csv])\n",
    "    row = stream.parse_csv(skip_lines=1)\n",
    "    return row.filter(partial(filter_fn,split))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(csv,unk_tkn=\"<unk>\"):\n",
    "    split = get_spilt(csv,\"train\")\n",
    "    \n",
    "    review_token = split.map(review_token_fn)    \n",
    "    review_vocab = torchtext.vocab.build_vocab_from_iterator(review_token,\n",
    "                                                             specials=[unk_tkn],min_freq=25)\n",
    "    review_vocab.set_default_index(review_vocab[unk_tkn])\n",
    "\n",
    "    rating_token = split.map(rating_token_fn)\n",
    "    rating_vocab = torchtext.vocab.build_vocab_from_iterator(rating_token)\n",
    "    \n",
    "    return review_vocab ,rating_vocab  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_vocab ,rating_vocab  = create_vocab(\"../data/reviews_with_splits_lite.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(review_vocab,rating_vocab,row):\n",
    "    review_vector = np.zeros(len(review_vocab))\n",
    "    review_vector[review_vocab.lookup_indices((get_review_token(row[1])))] = 1\n",
    "    \n",
    "    rating_vector = rating_vocab.lookup_indices([row[0]])[-1]\n",
    "    \n",
    "    return {\"x_data\":review_vector,\n",
    "            \"y_data\":rating_vector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(csv,split,review_vocab,rating_vocab):\n",
    "    split_iter = get_spilt(csv,split)\n",
    "    if split == \"train\":\n",
    "        split_iter = split_iter.shuffle()\n",
    "    return split_iter.map(partial(create_dataset,review_vocab,rating_vocab))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"../data/reviews_with_splits_lite.csv\"\n",
    "train_dataset = build_dataset(CSV_PATH,\"train\",review_vocab,rating_vocab)\n",
    "val_dataset = build_dataset(CSV_PATH,\"val\",review_vocab,rating_vocab)\n",
    "test_dataset = build_dataset(CSV_PATH,\"test\",review_vocab,rating_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
